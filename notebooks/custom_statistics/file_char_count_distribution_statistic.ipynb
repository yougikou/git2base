{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40da9bc",
   "metadata": {},
   "source": [
    "# FileCharCountDistributionStatistic 开发笔记\n",
    "\n",
    "本 notebook 演示如何开发 FileCharCount 字符区间统计卡片，并在快速上手框架中调试单次运行与趋势分析的呈现效果。\n",
    "\n",
    "**English:** This notebook demonstrates how to build the FileCharCount character-range statistic card and how to preview both single-run and trend outputs within the quickstart framework.  \n",
    "**日本語:** このノートブックでは、FileCharCount の文字数区間統計カードの作成方法と、クイックスタートフレームワーク内で単一実行とトレンドの表示を確認する方法を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bca768",
   "metadata": {},
   "source": [
    "## 数据准备\n",
    "\n",
    "运行下方单元以加载示例运行数据。根据实际情况调整 `repo_name` 和 `run_name`，以便查看不同执行结果。\n",
    "\n",
    "**English:** Run the cell below to load sample run data. Adjust `repo_name` and `run_name` as needed to inspect different executions.  \n",
    "**日本語:** 下のセルを実行してサンプルの実行データを読み込みます。必要に応じて `repo_name` と `run_name` を変更し、別の実行結果を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81304c1",
   "metadata": {
    "tags": [
     "skip-loader"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "NOTEBOOKS_DIR = Path.cwd().resolve().parent\n",
    "if str(NOTEBOOKS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(NOTEBOOKS_DIR))\n",
    "\n",
    "from quickstart_dashboard import RunDataLoader\n",
    "\n",
    "loader = RunDataLoader(base_dir=\"../../artifacts\")\n",
    "repos = loader.list_repos()\n",
    "\n",
    "if not repos:\n",
    "    print(\"⚠️ 未找到任何项目，请确认 ../../artifacts 目录存在分析结果。\")\n",
    "    repo_name = None\n",
    "    run_name = None\n",
    "    sample_run = None\n",
    "    sample_history = None\n",
    "else:\n",
    "    repo_name = repos[0]\n",
    "    print(f\"使用示例项目: {repo_name}\")\n",
    "    runs = loader.list_runs(repo_name)\n",
    "    if not runs:\n",
    "        print(\"⚠️ 项目下暂未找到运行记录。\")\n",
    "        run_name = None\n",
    "        sample_run = None\n",
    "        sample_history = None\n",
    "    else:\n",
    "        run_name = runs[0]\n",
    "        print(f\"使用示例运行: {run_name}\")\n",
    "        sample_run = loader.load_run(repo_name, run_name)\n",
    "        sample_history = loader.load_history(repo_name, limit=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630ef366",
   "metadata": {},
   "source": [
    "### 技术栈筛选\n",
    "\n",
    "选择需要查看的技术栈后，重新运行预览单元即可按栈过滤统计结果。\n",
    "\n",
    "**English:** Choose a tech stack and rerun the preview cells to filter the results for that stack.\n",
    "\n",
    "**日本語:** 技術スタックを選択し、プレビューセルを再実行すると、そのスタックに絞った結果を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6375d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "from quickstart_dashboard import RunData, TechStackClassifier\n",
    "\n",
    "\n",
    "stack_classifier = TechStackClassifier.from_config()\n",
    "stack_options = stack_classifier.stack_labels or [stack_classifier.all_label]\n",
    "default_stack = stack_options[0] if stack_options else None\n",
    "\n",
    "\n",
    "def filter_run_by_stack(run: RunData, stack_label: str | None) -> RunData:\n",
    "    \"\"\"Return a RunData copy filtered to the requested tech stack.\"\"\"\n",
    "\n",
    "    if run is None:\n",
    "        return run\n",
    "\n",
    "    label = stack_label or stack_classifier.all_label\n",
    "    if stack_classifier.is_all(label):\n",
    "        dataframes = dict(run.dataframes)\n",
    "    else:\n",
    "        dataframes = stack_classifier.filter_run_dataframes(run.dataframes, label)\n",
    "\n",
    "    metadata = dict(run.metadata or {})\n",
    "    metadata[\"selected_stack\"] = label\n",
    "    return RunData(\n",
    "        repo=run.repo,\n",
    "        run=run.run,\n",
    "        path=run.path,\n",
    "        metadata=metadata,\n",
    "        dataframes=dataframes,\n",
    "        timestamp=run.timestamp,\n",
    "        ended_at=run.ended_at,\n",
    "        selected_stack=label,\n",
    "    )\n",
    "\n",
    "\n",
    "stack_dropdown = widgets.Dropdown(\n",
    "    options=stack_options,\n",
    "    value=default_stack,\n",
    "    description=\"技术栈\",\n",
    "    layout=widgets.Layout(width=\"320px\"),\n",
    ")\n",
    "\n",
    "display(stack_dropdown)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e017574",
   "metadata": {},
   "source": [
    "## 定义统计卡片\n",
    "\n",
    "在下方代码单元中实现字符区间统计逻辑。运行后即可在当前会话中使用该类。\n",
    "\n",
    "**English:** Implement the character-range statistic in the code cell below. Once executed, the class becomes available for use in this session.  \n",
    "**日本語:** 以下のコードセルで文字数区間統計のロジックを実装します。セルを実行すると、このセッションでクラスを利用できるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0eee1",
   "metadata": {},
   "source": [
    "### 数据处理函数\n",
    "\n",
    "在定义统计类之前，我们先拆分出路径归一化和 diff 元数据匹配的辅助函数，方便单独运行并快速排查问题。\n",
    "**English:** Before defining the statistic class, split out helper functions for path normalization and diff metadata joins so they can be executed independently during debugging.\n",
    "**日本語:** 統計クラスを定義する前に、パスの正規化や差分メタデータ結合の補助関数を分離し、デバッグ時に個別に実行しやすくします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from quickstart_dashboard import RunData\n",
    "\n",
    "def normalize_paths(\n",
    "    df: pd.DataFrame, candidate_columns: Sequence[str] | None = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return a copy with normalized POSIX-style paths.\"\"\"\n",
    "\n",
    "    columns = list(candidate_columns or [])\n",
    "    if not columns:\n",
    "        columns = [col for col in (\"path\", \"file_path\") if col in df.columns]\n",
    "\n",
    "    if not columns:\n",
    "        result = df.copy()\n",
    "        result[\"path\"] = \"\"\n",
    "        return result\n",
    "\n",
    "    normalized = pd.Series([\"\"] * len(df), index=df.index, dtype=object)\n",
    "    for column in columns:\n",
    "        values = df[column].fillna(\"\").astype(str)\n",
    "        values = values.str.replace(\"\\\\\", \"/\", regex=False).str.strip(\"/\")\n",
    "        normalized = normalized.where(normalized != \"\", values)\n",
    "\n",
    "    result = df.copy()\n",
    "    result[\"path\"] = normalized\n",
    "    return result\n",
    "\n",
    "def load_char_counts(run: RunData) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Extract FileCharCount rows from analysis results.\"\"\"\n",
    "\n",
    "    df = run.dataframes.get(\"analysis_results_df\")\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "\n",
    "    if \"analyzer_type\" not in df.columns or \"count\" not in df.columns:\n",
    "        return None\n",
    "\n",
    "    filtered = df[df[\"analyzer_type\"] == \"FileCharCount\"].copy()\n",
    "    if filtered.empty:\n",
    "        return None\n",
    "\n",
    "    filtered[\"count\"] = pd.to_numeric(filtered[\"count\"], errors=\"coerce\")\n",
    "    filtered.dropna(subset=[\"count\"], inplace=True)\n",
    "    if filtered.empty:\n",
    "        return None\n",
    "\n",
    "    filtered = normalize_paths(filtered)\n",
    "    filtered[\"commit_hash\"] = filtered.get(\"commit_hash\", \"\").fillna(\"\").astype(str)\n",
    "    filtered = filtered[filtered[\"path\"] != \"\"].copy()\n",
    "    if filtered.empty:\n",
    "        return None\n",
    "\n",
    "    filtered.sort_values([\"path\", \"count\"], ascending=[True, False], inplace=True)\n",
    "    filtered = filtered.drop_duplicates(subset=[\"path\", \"commit_hash\"], keep=\"first\")\n",
    "    return filtered\n",
    "\n",
    "def attach_diff_metadata(\n",
    "    run: RunData, df: pd.DataFrame\n",
    ") -> tuple[pd.DataFrame, str, bool]:\n",
    "    \"\"\"Merge diff metadata to highlight added and changed files.\"\"\"\n",
    "\n",
    "    run_type = (run.metadata or {}).get(\"run_type\", \"\")\n",
    "    enriched = df.copy()\n",
    "    diff_info_available = False\n",
    "\n",
    "    if run_type == \"diff\":\n",
    "        diff_df = run.dataframes.get(\"diff_results_df\")\n",
    "        if diff_df is not None and not diff_df.empty:\n",
    "            diff_df = diff_df.copy()\n",
    "            for column in (\n",
    "                \"target_path\",\n",
    "                \"source_path\",\n",
    "                \"diff_change_type\",\n",
    "                \"target_commit_hash\",\n",
    "                \"base_commit_hash\",\n",
    "            ):\n",
    "                if column not in diff_df.columns:\n",
    "                    diff_df[column] = \"\"\n",
    "\n",
    "            diff_df[\"target_path\"] = diff_df[\"target_path\"].fillna(\"\").astype(str)\n",
    "            diff_df[\"source_path\"] = diff_df[\"source_path\"].fillna(\"\").astype(str)\n",
    "            diff_df[\"diff_change_type\"] = diff_df[\"diff_change_type\"].fillna(\"\").astype(str)\n",
    "            diff_df[\"target_commit_hash\"] = diff_df[\"target_commit_hash\"].fillna(\"\").astype(str)\n",
    "            diff_df[\"base_commit_hash\"] = diff_df[\"base_commit_hash\"].fillna(\"\").astype(str)\n",
    "\n",
    "            diff_df[\"_merge_path\"] = diff_df[\"target_path\"].where(\n",
    "                diff_df[\"target_path\"] != \"\", diff_df[\"source_path\"]\n",
    "            )\n",
    "            diff_df[\"_merge_path\"] = (\n",
    "                diff_df[\"_merge_path\"].str.replace(\"\\\\\", \"/\", regex=False).str.strip(\"/\")\n",
    "            )\n",
    "\n",
    "            diff_map = diff_df[\n",
    "                [\"_merge_path\", \"diff_change_type\", \"target_commit_hash\", \"base_commit_hash\"]\n",
    "            ].rename(columns={\"_merge_path\": \"path\"})\n",
    "\n",
    "            enriched = enriched.merge(diff_map, on=\"path\", how=\"left\")\n",
    "            enriched[\"diff_change_type\"] = (\n",
    "                enriched.get(\"diff_change_type\", \"\").fillna(\"\").astype(str).str.upper()\n",
    "            )\n",
    "            enriched[\"target_commit_hash\"] = (\n",
    "                enriched.get(\"target_commit_hash\", \"\").fillna(\"\").astype(str)\n",
    "            )\n",
    "            enriched[\"base_commit_hash\"] = (\n",
    "                enriched.get(\"base_commit_hash\", \"\").fillna(\"\").astype(str)\n",
    "            )\n",
    "\n",
    "            commit_hash = enriched.get(\"commit_hash\", \"\").fillna(\"\").astype(str)\n",
    "            target_mask = enriched[\"target_commit_hash\"] != \"\"\n",
    "            commit_match = commit_hash == enriched[\"target_commit_hash\"]\n",
    "            keep_mask = (~target_mask) | (target_mask & commit_match)\n",
    "            enriched = enriched.loc[keep_mask].copy()\n",
    "            enriched.sort_values([\"path\", \"count\"], ascending=[True, False], inplace=True)\n",
    "\n",
    "            diff_info_available = (\n",
    "                enriched[\"diff_change_type\"].replace(\"\", pd.NA).notna().any()\n",
    "            )\n",
    "        else:\n",
    "            enriched[\"diff_change_type\"] = pd.NA\n",
    "    else:\n",
    "        enriched[\"diff_change_type\"] = pd.NA\n",
    "\n",
    "    enriched = enriched.drop_duplicates(subset=[\"path\"], keep=\"first\")\n",
    "    return enriched, str(run_type), diff_info_available\n",
    "\n",
    "def bucket_label(value: float, bins: Sequence[tuple[str, int, Optional[int]]]) -> str:\n",
    "    \"\"\"Return the label matching the provided count.\"\"\"\n",
    "\n",
    "    for label, lower, upper in bins:\n",
    "        if value < lower:\n",
    "            continue\n",
    "        if upper is None or value < upper:\n",
    "            return label\n",
    "    return bins[-1][0]\n",
    "\n",
    "def summarize_file_char_run(\n",
    "    run: RunData, bins: Sequence[tuple[str, int, Optional[int]]]\n",
    ") -> Optional[dict[str, object]]:\n",
    "    \"\"\"Prepare summary rows for the statistic card.\"\"\"\n",
    "\n",
    "    base_df = load_char_counts(run)\n",
    "    if base_df is None:\n",
    "        return None\n",
    "\n",
    "    enriched, run_type, diff_info_available = attach_diff_metadata(run, base_df)\n",
    "    if enriched.empty:\n",
    "        return None\n",
    "\n",
    "    enriched = enriched.copy()\n",
    "    enriched[\"range_label\"] = enriched[\"count\"].apply(lambda value: bucket_label(value, bins))\n",
    "\n",
    "    stack_label = (run.metadata or {}).get(\"selected_stack\") or run.selected_stack or \"\"\n",
    "\n",
    "    rows: List[dict[str, object]] = []\n",
    "    for label, _, _ in bins:\n",
    "        subset = enriched[enriched[\"range_label\"] == label]\n",
    "        total = int(subset.shape[0])\n",
    "        added: Optional[int] = None\n",
    "        changed: Optional[int] = None\n",
    "\n",
    "        if run_type == \"diff\" and diff_info_available:\n",
    "            added = int(subset[\"diff_change_type\"].isin({\"A\"}).sum())\n",
    "            changed = int(subset[\"diff_change_type\"].isin({\"M\", \"R\"}).sum())\n",
    "\n",
    "        rows.append({\"range\": label, \"total\": total, \"added\": added, \"changed\": changed})\n",
    "\n",
    "    return {\n",
    "        \"rows\": rows,\n",
    "        \"run_type\": str(run_type),\n",
    "        \"diff_info\": diff_info_available,\n",
    "        \"stack_label\": stack_label,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7193520",
   "metadata": {},
   "source": [
    "### 统计卡片类\n",
    "\n",
    "接下来利用这些辅助函数实现最终的统计卡片类，便于在 Notebook 中调试并导出到仪表盘。\n",
    "**English:** Next, use these helpers to implement the final statistic class so it can be debugged in the notebook and exported to the dashboard.\n",
    "**日本語:** これらの補助関数を利用して最終的な統計クラスを実装し、ノートブックでデバッグしダッシュボードへエクスポートできるようにします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc5c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from quickstart_dashboard import BaseStatistic, RunData, RunHistory, _card_container\n",
    "\n",
    "\n",
    "class FileCharCountDistributionStatistic(BaseStatistic):\n",
    "    \"\"\"Bucket files by character count and highlight diff additions/modifications.\"\"\"\n",
    "\n",
    "    BINS: Sequence[tuple[str, int, Optional[int]]] = (\n",
    "        (\"<1k\", 0, 1_000),\n",
    "        (\"1k-2k\", 1_000, 2_000),\n",
    "        (\"2k-3k\", 2_000, 3_000),\n",
    "        (\"3k-10k\", 3_000, 10_000),\n",
    "        (\"10k-50k\", 10_000, 50_000),\n",
    "        (\">=50k\", 50_000, None),\n",
    "    )\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.name = \"文件字符区间\"\n",
    "        self.description = \"基于 FileCharCount 分析结果统计字符区间，并区分新增/变更文件数量\"\n",
    "\n",
    "    def render_single(self, run: RunData) -> widgets.Widget:\n",
    "        summary = summarize_file_char_run(run, self.BINS)\n",
    "        if summary is None:\n",
    "            return _card_container(\n",
    "                self.name,\n",
    "                description=self.description,\n",
    "                body_html=\"<div style='color:#666;'>暂无 FileCharCount 分析结果可用于统计。</div>\",\n",
    "                min_width=\"360px\",\n",
    "            )\n",
    "\n",
    "        diff_run = summary[\"run_type\"] == \"diff\"\n",
    "        diff_info = diff_run and summary[\"diff_info\"]\n",
    "        if diff_info:\n",
    "            note = \"统计基于 diff 运行匹配的 FileCharCount 结果。\"\n",
    "        elif diff_run:\n",
    "            note = \"diff 运行未匹配到文件级变更，新增/变更列以 “-” 显示。\"\n",
    "        else:\n",
    "            run_label = summary[\"run_type\"] or \"未知\"\n",
    "            note = f\"当前运行类型为 {run_label}，新增/变更列以 “-” 显示。\"\n",
    "\n",
    "        stack_label = summary.get(\"stack_label\") or \"\"\n",
    "        stack_note = \"\"\n",
    "        if stack_label:\n",
    "            stack_note = f\"技术栈筛选：{stack_label}\"\n",
    "\n",
    "        table = self._render_distribution_table(summary[\"rows\"], diff_info)\n",
    "\n",
    "        body_widgets = [\n",
    "            widgets.HTML(value=f\"<div style='color:#666;font-size:12px;'>{note}</div>\"),\n",
    "        ]\n",
    "        if stack_note:\n",
    "            body_widgets.append(\n",
    "                widgets.HTML(value=f\"<div style='color:#666;font-size:12px;'>{stack_note}</div>\")\n",
    "            )\n",
    "        body_widgets.append(table)\n",
    "\n",
    "        return _card_container(\n",
    "            self.name,\n",
    "            description=self.description,\n",
    "            body_widgets=body_widgets,\n",
    "            min_width=\"360px\",\n",
    "        )\n",
    "\n",
    "    def _render_distribution_table(\n",
    "        self, rows: Sequence[dict[str, object]], diff_info: bool\n",
    "    ) -> widgets.HTML:\n",
    "        header_cells = [\"区间\", \"文件数\", \"新增\", \"变更\"]\n",
    "        html = [\n",
    "            \"<table style='border-collapse:collapse;font-size:12px;width:100%;max-width:520px;'>\",\n",
    "            \"<thead><tr>\",\n",
    "        ]\n",
    "        for cell in header_cells:\n",
    "            html.append(\n",
    "                f\"<th style='border-bottom:1px solid #ddd;padding:6px 8px;text-align:left;color:#555;font-weight:600;'>{cell}</th>\"\n",
    "            )\n",
    "        html.append(\"</tr></thead><tbody>\")\n",
    "\n",
    "        for row in rows:\n",
    "            total = row[\"total\"]\n",
    "            added = row.get(\"added\")\n",
    "            changed = row.get(\"changed\")\n",
    "            html.append(\"<tr>\")\n",
    "            html.append(\n",
    "                f\"<td style='padding:6px 8px;border-bottom:1px solid #f0f0f0;color:#333;'>{row['range']}</td>\"\n",
    "            )\n",
    "            html.append(\n",
    "                f\"<td style='padding:6px 8px;border-bottom:1px solid #f0f0f0;color:#333;'>{total}</td>\"\n",
    "            )\n",
    "            if diff_info:\n",
    "                html.append(\n",
    "                    f\"<td style='padding:6px 8px;border-bottom:1px solid #f0f0f0;color:#333;'>{added if added is not None else 0}</td>\"\n",
    "                )\n",
    "                html.append(\n",
    "                    f\"<td style='padding:6px 8px;border-bottom:1px solid #f0f0f0;color:#333;'>{changed if changed is not None else 0}</td>\"\n",
    "                )\n",
    "            else:\n",
    "                html.append(\"<td style='padding:6px 8px;border-bottom:1px solid #f0f0f0;color:#999;'>-</td>\")\n",
    "                html.append(\"<td style='padding:6px 8px;border-bottom:1px solid #f0f0f0;color:#999;'>-</td>\")\n",
    "            html.append(\"</tr>\")\n",
    "\n",
    "        html.append(\"</tbody></table>\")\n",
    "        return widgets.HTML(value=\"\".join(html))\n",
    "\n",
    "    def render_trend(self, history: RunHistory) -> widgets.Widget:\n",
    "        stack_label = \"\"\n",
    "        if history.runs:\n",
    "            stack_label = history.runs[-1].selected_stack or \"\"\n",
    "\n",
    "        hint = \"该统计不支持趋势分析。\"\n",
    "        if stack_label:\n",
    "            hint += f\" 当前技术栈筛选：{stack_label}。\"\n",
    "\n",
    "        return _card_container(\n",
    "            self.name,\n",
    "            description=self.description,\n",
    "            body_html=f\"<div style='color:#666;'>{hint}</div>\",\n",
    "            min_width=\"360px\",\n",
    "            flex=\"1 1 100%\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e80421",
   "metadata": {},
   "source": [
    "## 调试与预览\n",
    "\n",
    "运行以下单元，在 notebook 中直接查看单次运行卡片和趋势统计，便于上线前快速验证。\n",
    "\n",
    "**English:** Execute the following cells to preview the single-run card and trend statistics directly inside the notebook, making it easy to validate before publishing.  \n",
    "**日本語:** 次のセルを実行すると、ノートブック上で単一実行カードとトレンド統計を直接確認でき、公開前の検証が容易になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04180687",
   "metadata": {
    "tags": [
     "skip-loader"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "if sample_run is None:\n",
    "    print(\"⚠️ 没有可用的运行数据，无法预览单次统计卡片。\")\n",
    "else:\n",
    "    stat = FileCharCountDistributionStatistic()\n",
    "    stack_label = stack_dropdown.value if \"stack_dropdown\" in globals() else None\n",
    "    preview_run = filter_run_by_stack(sample_run, stack_label)\n",
    "    widget = stat.render_single(preview_run)\n",
    "    display(widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28713a84",
   "metadata": {
    "tags": [
     "skip-loader"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "if sample_history is None or not sample_history.runs:\n",
    "    print(\"⚠ 没有足够的历史数据，无法绘制趋势图。\")\n",
    "else:\n",
    "    stat = FileCharCountDistributionStatistic()\n",
    "    stack_label = stack_dropdown.value if \"stack_dropdown\" in globals() else None\n",
    "    filtered_runs = [\n",
    "        filter_run_by_stack(run, stack_label)\n",
    "        for run in sample_history.runs\n",
    "    ]\n",
    "    filtered_history = RunHistory(repo=sample_history.repo, runs=filtered_runs)\n",
    "    widget = stat.render_trend(filtered_history)\n",
    "    display(widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd423fd",
   "metadata": {},
   "source": [
    "## 导出到仪表盘\n",
    "\n",
    "确认逻辑后，可以在 `quickstart_dashboard.ipynb` 中通过 `load_statistic_from_notebook(\"custom_statistics/file_char_count_distribution_statistic.ipynb\")` 将此统计类加载并注册到仪表盘。\n",
    "\n",
    "**English:** After validating the logic, load and register this statistic in `quickstart_dashboard.ipynb` via `load_statistic_from_notebook(\"custom_statistics/file_char_count_distribution_statistic.ipynb\")`.  \n",
    "**日本語:** ロジックを確認したら、`quickstart_dashboard.ipynb` で `load_statistic_from_notebook(\"custom_statistics/file_char_count_distribution_statistic.ipynb\")` を使ってこの統計クラスを読み込み、ダッシュボードに登録してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
